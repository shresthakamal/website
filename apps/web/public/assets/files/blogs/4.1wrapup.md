
# Handling Longer Sequences in Transformers

-   Models in Transformers can only handle to 521 ot 1024 tokens in a sequence.
-   What about longer tokens?
    -   Solution: **Truncation**
        ```
        max_sequence_length = some value
        sequence = sequence[:max_sequence_length]
        ```
-   Now with this we have input_ids, attention_mask, truncation, padding DONE.
-   All of these things are handled by the tokenizer itself. SO no need to worry about it.

```
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)

---
{
    'input_ids': [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], 
    'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
    }

```
-   We can do the same for multiple sentences as well, So the `tokenizer` function is a ver powerful function.

```
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)
---

{
    'input_ids': [
        [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], 
        [101, 2061, 2031, 1045, 999, 102]], 
    'attention_mask': [
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
        [1, 1, 1, 1, 1, 1]]}

```

-   Problem with multiple sentences is: Non Uniform length of the sequences

    -   Solution1: Padding
        -   Most trivial way of padding: Pad to the longest sequences 

            ```model_inputs = tokenizer(sequences, padding="longest")```
        
        -   Maximum Length padding
            Pad to maximum length based on the model: [distilbert = 512]

            ```model_inputs = tokenizer(sequences, padding="max_length")```

        -   Padding to a specific length

            ```model_inputs = tokenizer(sequences, padding="max_length", max_length=8)```

    
    -   Solution2: Truncation
        -   Will truncate the sequences that are more than the specfied maximum length
        ```
        model_inputs = tokenizer(sequences, truncation=True)

        model_inputs = tokenizer(sequences, max_length=8, truncation=True)
        ```

-   With that we have solved the problem of the longer sequences
-   **Multiple Sequences ==> PADDING**
-   **Longer Sequences ==> TRUNCATION**


## Special Tokens
```
sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])
---
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]




print(tokenizer.decode(ids))
print(tokenizer.decode(model_inputs["input_ids"]))
---
i've been waiting for a huggingface course my whole life.
[CLS] i've been waiting for a huggingface course my whole life. [SEP]
```
-   As we can see from the above code, the tokenizer has added some extra tokens: `101` and `102`
-   Now if we decode the tokens and see, we have two new special tokens `[CLS], [SEP]` that marks the begining of the sequence and the end of a sequence respectively.
    - Why did we add these?
        -   Becasue the models were trained that way = so to get the same results, we need to add them
    -   Note: _Some models dont add special tokens, some add different tokens: Tokenizer knows and deals it for you._


## Wrapping everything together:

```
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

output = model(**tokens)

---
SequenceClassifierOutput([('logits', tensor([[-1.5607,  1.6123],
                                   [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>))])

```