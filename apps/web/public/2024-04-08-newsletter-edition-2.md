---
layout: post
title: LLM News and Articles Weekly Digest - April 15, 2024
date: 2024-04-15 15:09:00
description: Weekly Updates on advancements in LLMs 
categories: nlp llm genai newsletter weeklydigest
disqus_comments: true
related_posts: true
---

<center><img src="/assets/newsletter/april-15/1.jpg" width=900px height=400px><br><br><p>Photo by <a href="https://unsplash.com/@sigmund">Sigmund</a> on Unsplash</p></center>

<br>


## Latest News

1. <b><u>x.AI Unveils its First Multimodal model, Grok-1.5 Vision</u></b> <br> x.AI has announced that its latest flagship model has vision capabilities on par with (and in some cases exceeding) state-of-the-art models. [[Source]](https://www.maginative.com/article/x-ai-unveils-its-first-multimodal-model-grok-1-5-vision/)

2. <b><u>OpenAI Fires Researchers For Leaking Information</u></b> <br> 
OpenAI has reportedly fired two researchers who were allegedly linked to the leaking of company secrets following months of leaks and company efforts to crack down on such incidents. [[Source]](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffuturism.com%2Fthe-byte%2Fopenai-fires-researchers-leaks%3Futm_source=tldrai/1/0100018ee1ea6a3b-761fbb14-134a-49d5-8ab2-8bd6e500a559-000000/ONTPPOcKDt8rR6rEZDuTguPHeqEflNj88g62qIt5hOM=348)

3. <b><u>Cohere Launches New Rerank 3 Model</u></b> <br> 
This adaptable model seamlessly meshes with various databases or search indexes and effortlessly integrates into older applications boasting native search capabilities. With the insertion of a mere line of code, Rerank 3 can amplify search effectiveness or slash the costs associated with running RAG applications, all while keeping latency to a minimum. [[Source]](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftxt.cohere.com%2Frerank-3%2F%3Futm_source=tldrai/1/0100018ed2776a31-5d471466-9c08-425b-a2b5-b923ab91dd14-000000/ck9F1USw1pdHYiY6dbGuJqD_HdS-ldo5Y-O_ppKKJBw=348)

4. <b><u>Google’s Gemini Pro 1.5 Enters Public Preview</u></b><br> Google has made its most advanced generative AI model, Gemini 1.5 Pro, available in public preview on its Vertex AI platform. It has a context window of 1 million tokens, can understand audios, has a JSON mode for devs and acts on your commands.
[[Source]](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2024%2F04%2F09%2Fgoogles-gemini-pro-1-5-enters-public-preview-on-vertex-ai%2F%3Futm_source=tldrai/1/0100018ecd56ca0e-153c9525-6830-4d31-8024-d652509500b9-000000/2PU53uA9-rczVX-piXr-6ntNkDpRosp_WTmTc5G1Ve4=348)

5. <b><u>Mistral releases Mixtral 8x22 Apache 2 licensed MoE model</u></b><br> A new 8x22B model like always with a magnet link. Initial community benchmarks indicate that the first version performs impressively as a base model, boasting 77 MMLU (typically linked with reasoning tasks).
[[Source]](https://twitter.com/MistralAI/status/1777869263778291896)


6. <b><u>Meta Confirms That Llama 3 Is Coming Next Month — GPT 4 Competitor?</u></b><br> Meta has confirmed plans to release Llama 3, the next generation of its large language model for generative AI assistants, within the next month.
[[Source]](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2024%2F04%2F09%2Fmeta-confirms-that-its-llama-3-open-source-llm-is-coming-in-the-next-month%2F%3Futm_source=tldrai/1/0100018ec82a8af8-7a1b345e-7b10-4120-b7a6-a335ccb5ecfb-000000/prN8GN6IDh6KGEIO1Se7p-zqGcumtDXHeu38SAwhxFc=347)

<br>

## Articles

1. [Lessons after a half-billion GPT tokens](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkenkantzer.com%2Flessons-after-a-half-billion-gpt-tokens%2F%3Futm_source=tldrnewsletter/1/0100018ee153c4a6-e226915e-c04d-4b21-a301-4a8637d4823f-000000/uKCsJpNZgQBg7vKzgF-KP0WadCphJDgybP1B2HEbD8Y=348)

2. [The State of Generative AI, 2024](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.thealgorithmicbridge.com%2Fp%2Fthe-state-of-generative-ai-2024%3Futm_source=tldrai/1/0100018ed2776a31-5d471466-9c08-425b-a2b5-b923ab91dd14-000000/g8cEmMP47ehgWtYTKruY603tvVcn6KSAFIKJZ5H09WA=348)

3. [Folks at LlamaIndex have launched the LlamaIndex + MistralAI Cookbook Series for creating a range of RAG applications](hhttps://github.com/mistralai/cookbook/tree/main/third_party/LlamaIndex)

4. [The Fears and Opportunities of AI Written Content](https://pub.towardsai.net/the-fears-and-opportunities-of-ai-written-content-ae77fc6e4f74)

<br>


## Papers and Repositories

1. <b><u>Evaluating Large Language Models on Long Texts</u></b><br>
Ada-LEval is a groundbreaking benchmark for assessing long-context capabilities with adaptable-length questions. It includes two challenging tasks: TSort, arranging text segments, and BestAnswer, selecting the best answer from multiple candidates. [[Source]](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fopen-compass%2Fada-leval%3Futm_source=tldrai/1/0100018ecd56ca0e-153c9525-6830-4d31-8024-d652509500b9-000000/uETwLDwgHegSvDxABTTa6X4vTOva0AqIx30lcC0U6N4=348)

2. <b><u>karpathy/llm.c: LLM training in simple, raw C/CUDA.</u></b><br>
Karpathy’s project focuses on developing a minimalist GPT-2 training framework using C/CUDA, aiming to replicate the PyTorch model in around 1,000 lines of code while enhancing performance through direct CUDA integration and tailored CPU optimizations.[[Source]](https://github.com/karpathy/llm.c)

3. <b><u>Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs.</u></b><br>
Apple researchers have created Ferret-UI, an advanced multimodal large language model (MLLM) tailored for enhanced interpretation and interaction with mobile user interface (UI) screens. [[Source]](https://arxiv.org/abs/2404.05719)

4. <b><u>Rho-1: Not All Tokens Are What You Need.</u></b><br>
The authors analyze token importance in language model training, revealing varied loss patterns. This leads to RHO-1, a new model using Selective Language Modeling (SLM) to focus on training with beneficial tokens, rather than treating all equally. [[Source]](https://arxiv.org/abs/2404.07965)

5. <b><u>Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention.</u></b><br>
The work introduces Infini-attention, an attention mechanism within a Transformer block, enabling LLMs to handle infinitely long inputs while maintaining bounded memory and computational requirements. [[Source]](https://arxiv.org/abs/2404.07143)

<br>

Thank you for reading ! 

If you have any suggestions or feedback, please do comment. You can find me on [[Linkedin]](https://www.linkedin.com/in/shresthakamal/).

Find the medium post here: https://shresthakamal.medium.com/llm-news-and-articles-weekly-digest-april-8-2024-466fe73f6233.

Do subscribe for future newsletters.


